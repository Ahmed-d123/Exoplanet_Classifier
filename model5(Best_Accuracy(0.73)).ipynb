{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38574f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(1.611076016003369), 1: np.float64(1.1608253679259597), 2: np.float64(0.6588306208559374)}\n",
      "\n",
      "=== Test Set Classification Report ===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CANDIDATE       0.48      0.52      0.50       396\n",
      "     CONFIRMED       0.72      0.80      0.76       549\n",
      "FALSE POSITIVE       0.86      0.78      0.82       968\n",
      "\n",
      "      accuracy                           0.73      1913\n",
      "     macro avg       0.69      0.70      0.69      1913\n",
      "  weighted avg       0.74      0.73      0.73      1913\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[204  98  94]\n",
      " [ 80 440  29]\n",
      " [139  75 754]]\n",
      "✅ Saved ensemble pipeline for deployment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PYTHON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Ensemble: XGBoost + CatBoost + LightGBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "# Tree-based models\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 1) Load & clean data\n",
    "\n",
    "df = pd.read_csv(\"kepler_koi.csv\", comment=\"#\")\n",
    "\n",
    "features = [\n",
    "    'koi_period', 'koi_prad', 'koi_sma', 'koi_incl',\n",
    "    'koi_teq', 'koi_slogg', 'koi_srad', 'koi_smass', 'koi_steff'\n",
    "]\n",
    "\n",
    "df_clean = df[features + ['koi_disposition']].copy()\n",
    "\n",
    "# Fill NaNs\n",
    "for col in features:\n",
    "    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "# Feature engineering\n",
    "df_clean['density_star'] = df_clean['koi_smass'] / (df_clean['koi_srad']**3 + 1e-6)\n",
    "df_clean['prad_ratio']   = df_clean['koi_prad'] / (df_clean['koi_srad'] + 1e-6)\n",
    "df_clean['period_ratio'] = df_clean['koi_period'] / (df_clean['koi_sma'] + 1e-6)\n",
    "df_clean['teq_scaled']   = df_clean['koi_teq'] / (df_clean['koi_steff'] + 1e-6)\n",
    "\n",
    "# Extra features \n",
    "extra_features = ['density_star', 'prad_ratio', 'period_ratio', 'teq_scaled']\n",
    "\n",
    "\n",
    "all_features = features + extra_features\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_clean[all_features])\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_clean['koi_disposition'])\n",
    "\n",
    "# 2) Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Compute class weights\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# 4) Define individual models\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "    eval_metric='mlogloss', use_label_encoder=False\n",
    ")\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500, depth=6, learning_rate=0.05,\n",
    "    loss_function='MultiClass', class_weights=class_weights_dict,\n",
    "    eval_metric='TotalF1', random_seed=42, verbose=0\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    class_weight='balanced', random_state=42\n",
    ")\n",
    "\n",
    "# 5) Ensemble with soft voting\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('cat', cat_model), ('lgb', lgb_model)],\n",
    "    voting='soft', n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6) Train ensemble\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# 7) Predict & evaluate\n",
    "\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Test Set Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 8) Save ensemble & preprocessor\n",
    "\n",
    "joblib.dump(ensemble, \"ensemble_pipeline.joblib\")\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "joblib.dump(le, \"label_encoder.joblib\")\n",
    "print(\"✅ Saved ensemble pipeline for deployment\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
